---
title: 深度学习知识点
date: 2017-09-13 22:05:39
tags: 技术
---
* 玻尔兹曼机

* 过拟合：偏差很小，但方差很大（所以欠拟合叫做高偏差，过拟合叫做高方差）
训练数据太少，模型参数太多
解决方法：1. 剔除不必要的模型参数 2. 使用正则化（regularizaion），保留模型参数，但是减小模型参数的数量级（对某些参数，在loss函数中添加惩罚项）

* L2正则项
解决过拟合的问题
https://www.zhihu.com/question/20924039

* 交叉验证
https://baike.baidu.com/item/%E4%BA%A4%E5%8F%89%E9%AA%8C%E8%AF%81
将数据集分成多个多个小的数据集，先用小数据集1训练，用小数据集2验证（评估正则化函数），直到最后一个小的数据集被用于验证

* 极大似然估计
http://www.jianshu.com/p/f1d3906e4a3e

* 激活函数
在神经网络中引入非线性的成分
sigmod：0，1之间
tanh：-1，1之间
softmax: 多分类，一般用在输出层的激活

* epoch、 iteration和batchsize
http://blog.csdn.net/sinat_30071459/article/details/50721565

* 交叉熵
https://zhuanlan.zhihu.com/p/27223959
在机器学习中，可以用交叉熵来定义loss function

* BP神经网络
反向传播计算——BackPropagation
单个参数：http://www.cnblogs.com/charlotte77/p/5629865.html
矩阵计算：http://www.jeyzhang.com/cnn-learning-notes-2.html

* 卷积神经网络CNN
局部连接，参数共享，卷积，池化等概念
介绍：http://www.jeyzhang.com/cnn-learning-notes-1.html
模型训练：http://www.jeyzhang.com/cnn-learning-notes-2.html

* 局部对比归一化
Local contrast normalization
http://blog.csdn.net/zouxy09/article/details/10007237

* batch normalization
http://blog.csdn.net/hjimce/article/details/50866313
http://minibatch.net/2017/06/11/%E7%BB%8F%E5%85%B8%E8%AE%BA%E6%96%87-Batch-Normalization/
https://zhuanlan.zhihu.com/p/26682707
反向计算梯度：http://mlnote.com/2016/12/20/Neural-Network-Batch-Normalization-and-Caffe-Code/

Good:https://kratzert.github.io/2016/02/12/understanding-the-gradient-flow-through-the-batch-normalization-layer.html
* 白化
http://blog.csdn.net/hjimce/article/details/50864602
http://blog.csdn.net/whiteinblue/article/details/36171233
